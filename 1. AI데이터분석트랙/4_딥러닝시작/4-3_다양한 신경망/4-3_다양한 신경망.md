### 4-3. 다양한 신경망



**MNIST 분류 CNN 모델**

- 신경망을 이용한 학습을 시작할 때 대부분 MNIST를 접하게 됩니다. MNIST는 손글씨로 된 사진을 모아 둔 데이터입니다.
- 손으로 쓴 0부터 9까지의 글자들이 있고, 이 데이터를 사용해서 신경망을 학습시키고, 학습 결과가 손글씨를 인식할 수 있는지 검증합니다.

![mnist](.\mnist.png)

1. 데이터 전처리하기 

> * 이번 실습에서는 우선 이미지 데이터를 출력하고 그 형태를 확인하여 CNN 모델에 적용할 수 있도록 데이터 전 처리를 수행합니다.
> * CNN을 위한 데이터 전처리
>
> > * MNIST 데이터는 이미지 데이터이지만 가로 길이와 세로 길이만 존재하는 2차원 데이터입니다. CNN 모델은 채널(RGB 혹은 흑백)까지 고려한 3차원 데이터를 입력으로 받기에 채널 차원을 추가해 데이터의 모양(shape)을 바꿔줍니다. 
> >
> > > [데이터 수, 가로 길이, 세로 길이]
> > > -> [데이터 수, 가로 길이, 세로 길이, 채널 수
> >
> > * 차원추가함수
> >
> > ```python
> > tf.expand_dims(data, axis)
> > ```
> >
> > > Tensor 배열 데이터에서 마지막 축(axis)에 해당하는 곳에 차원 하나를 추가할 수 있는 코드입니다. ( axis에 -1을 넣으면 어떤 `data`가 들어오던 마지막 축의 index를 의미합니다.)

```python
'''
<목표>
학습용 및 평가용 데이터를 CNN 모델의 입력으로 사용할 수 있도록 
(샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.
- tf.expand_dims 함수를 활용하여 train_images, test_images 데이터의 형태를 변환하고 각각   
  train_images, test_images에 저장합니다.
'''

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from elice_utils import EliceUtils

elice_utils = EliceUtils()

import logging, os
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# 동일한 실행 결과 확인을 위한 코드입니다.
np.random.seed(123)
tf.random.set_seed(123)


# MNIST 데이터 세트를 불러옵니다.
mnist = tf.keras.datasets.mnist

# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    

# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.
train_images, train_labels = train_images[:5000], train_labels[:5000]
test_images, test_labels = test_images[:1000], test_labels[:1000]


print("원본 학습용 이미지 데이터 형태: ",train_images.shape)
print("원본 평가용 이미지 데이터 형태: ",test_images.shape)
print("원본 학습용 label 데이터: ",train_labels)

# 첫 번째 샘플 데이터를 출력합니다.
plt.figure(figsize=(10, 10))
plt.imshow(train_images[0], cmap=plt.cm.binary)
plt.colorbar()
plt.title("Training Data Sample")
plt.savefig("sample1.png")
elice_utils.send_image("sample1.png")

# 9개의 학습용 샘플 데이터를 출력합니다.
class_names = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.savefig("sample2.png")
elice_utils.send_image("sample2.png")

"""
1. CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.
"""
train_images = tf.expand_dims(train_images, -1)
test_images = tf.expand_dims(test_images, -1)

print("변환한 학습용 이미지 데이터 형태: ",train_images.shape)
print("변환한 평가용 이미지 데이터 형태: ",test_images.shape)

'''
원본 학습용 이미지 데이터 형태:  (5000, 28, 28)
원본 평가용 이미지 데이터 형태:  (1000, 28, 28)
원본 학습용 label 데이터:  [5 0 4 ... 2 1 2]
변환한 학습용 이미지 데이터 형태:  (5000, 28, 28, 1)
변환한 평가용 이미지 데이터 형태:  (1000, 28, 28, 1)
'''
```

![sample1](.\sample1.png)![sample2](.\sample2.png)

2. 모델구현 

> *  이번 실습에서는 CNN 모델을 구현하고 학습해보겠습니다.
> * Keras에서 CNN 모델을 만들기 위해 필요한 함수/메서드
>
> > * CNN 레이어
> >
> > ```python
> > tf.keras.layers.Conv2D(filters, kernel_size, activation, padding)
> > ```
> >
> > > 입력 이미지의 특징, 즉 처리할 특징 맵(map)을 추출하는 레이어입니다.
> > >
> > > * filters : 필터(커널) 개수
> > > * kernel_size : 필터(커널)의 크기
> > > * activation : 활성화 함수
> > > * padding : 이미지가 필터를 거칠 때 그 크기가 줄어드는 것을 방지하기 위해서 가장자리에 0의 값을 가지는 픽셀을 넣을 것인지 말 것인지를 결정하는 변수. ‘SAME’ 또는 ‘VALID’
> >
> > * Maxpool 레이어
> >
> > ```python
> > tf.keras.layers.MaxPool2D(padding)
> > ```
> >
> > > 처리할 특징 맵(map)의 크기를 줄여주는 레이어입니다.
> > >
> > > * padding : ‘SAME’ 또는 ‘VALID’
> >
> > * Flatten 레이어
> >
> > ```python
> > tf.keras.layers.Flatten()
> > ```
> >
> > >  Convolution layer 또는 MaxPooling layer의 결과는 N차원의 텐서 형태입니다. 이를 1차원으로 평평하게 만들어줍니다.
> >
> > * Dense 레이어
> >
> > ```python
> > tf.keras.layers.Dense(node, activation)
> > ```
> >
> > > * node : 노드(뉴런) 개수
> > > * activation : 활성화 함수

```python
'''
<목표>
keras를 활용하여 CNN 모델을 설정합니다.
분류 모델에 맞게 마지막 레이어의 노드 수는 10개, activation 함수는 ‘softmax’로 설정합니다.
'''

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from visual import *
from elice_utils import EliceUtils

elice_utils = EliceUtils()

import logging, os
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# 동일한 실행 결과 확인을 위한 코드입니다.
np.random.seed(123)
tf.random.set_seed(123)


# MNIST 데이터 세트를 불러옵니다.
mnist = tf.keras.datasets.mnist

# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    

# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.
train_images, train_labels = train_images[:5000], train_labels[:5000]
test_images, test_labels = test_images[:1000], test_labels[:1000]

# CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.
train_images = tf.expand_dims(train_images, -1)
test_images = tf.expand_dims(test_images, -1)


"""
1. CNN 모델을 설정합니다.
   분류 모델에 맞게 마지막 레이어의 노드 수는 10개, activation 함수는 'softmax'로 설정합니다.
"""
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME', input_shape = (28,28,1)),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation = 'relu'),
    tf.keras.layers.Dense(10, activation = 'softmax')
])

# CNN 모델 구조를 출력합니다.
print(model.summary())

# CNN 모델의 학습 방법을 설정합니다.
model.compile(loss = 'sparse_categorical_crossentropy',
              optimizer = 'adam',

              metrics = ['accuracy'])
              
# 학습을 수행합니다. 
history = model.fit(train_images, train_labels, epochs = 10, batch_size = 128)


# 학습 결과를 출력합니다.
Visulaize([('CNN', history)], 'loss')
```

![learning_result](.\learning_result.png)

3. 평가 및 예측 

> * 이어서 이번 실습에서는 CNN 모델을 평가하고 예측해보겠습니다.
> * Keras에서 CNN 모델의 평가 및 예측을 위해 필요한 함수/메서드
>
> > * 평가 방법
> >
> > ```python
> > model.evaluate(X, Y)
> > ```
> >
> > > `evaluate()` 메서드는 학습된 모델을 바탕으로 입력한 feature 데이터 `X`와 label `Y`의 loss 값과 metrics 값을 출력합니다.
> >
> > * 예측 방법
> >
> > ```python
> > model.predict_classes(X)
> > ```
> >
> > > `X` 데이터의 예측 label 값을 출력합니다.

```python
'''
<목표>
1. evaluate 메서드와 평가용 데이터를 사용하여 모델을 평가합니다.
 - loss와 accuracy를 계산하고 loss, test_acc에 저장합니다.
2. predict_classes 메서드를 사용하여 평가용 데이터에 대한 예측 결과를 predictions에 저장합니다.
'''
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from visual import *
from plotter import *
from elice_utils import EliceUtils

elice_utils = EliceUtils()

import logging, os
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# 동일한 실행 결과 확인을 위한 코드입니다.
np.random.seed(123)
tf.random.set_seed(123)


# MNIST 데이터 세트를 불러옵니다.
mnist = tf.keras.datasets.mnist

# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    

# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.
train_images, train_labels = train_images[:5000], train_labels[:5000]
test_images, test_labels = test_images[:1000], test_labels[:1000]

# CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.
train_images = tf.expand_dims(train_images, -1)
test_images = tf.expand_dims(test_images, -1)


# CNN 모델을 설정합니다.
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME', input_shape = (28,28,1)),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation = 'relu'),
    tf.keras.layers.Dense(10, activation = 'softmax')
])

# CNN 모델 구조를 출력합니다.
print(model.summary())

# CNN 모델의 학습 방법을 설정합니다.
model.compile(loss = 'sparse_categorical_crossentropy',
              optimizer = 'adam',

              metrics = ['accuracy'])
              
# 학습을 수행합니다. 
history = model.fit(train_images, train_labels, epochs = 10, batch_size = 128, verbose = 2)


Visulaize([('CNN', history)], 'loss') #'①번 그림' 참조

"""
1. 평가용 데이터를 활용하여 모델을 평가합니다.
   loss와 accuracy를 계산하고 loss, test_acc에 저장합니다.
"""
loss, test_acc = model.evaluate(test_images, test_labels, verbose = 2)

"""
2. 평가용 데이터에 대한 예측 결과를 predictions에 저장합니다.
"""
predictions = model.predict_classes(test_images)

# 모델 평가 및 예측 결과를 출력합니다.
print('\nTest Loss : {:.4f} | Test Accuracy : {}'.format(loss, test_acc))
print('예측한 Test Data 클래스 : ',predictions[:10])

# 평가용 데이터에 대한 레이어 결과를 시각화합니다.
Plotter(test_images, model) # '②번 그림' 참조

'''
(중략)
Test Loss : 0.1806 | Test Accuracy : 0.949999988079071
예측한 Test Data 클래스 :  [7 2 1 0 4 1 4 9 5 9]
'''
```

> ①번 그림(학습경과 시각화)

![learning_result2](.\learning_result2.png)

> ②번 그림 (평가용 데이터에 대한 레이어 결과)

![test_images1](.\test_images1.png)

![test_images2](.\test_images2.png)

![test_images3](.\test_images3.png)

![test_images4](.\test_images4.png)



**MNIST 분류 - MLP vs CNN**

> * CNN은 이미지 데이터를 특히 잘 다룰 수 있는 신경망 모델이라고 배웠습니다. 그렇다면, 실제로 MNIST 분류 task를 MLP와 CNN으로 각각 학습하였을 때의 성능 차이를 비교해보겠습니다.
>
> * 동등한 비교를 위해 CNN의 weight의 수 (52,298개)와 MLP의 weight의 수(52,650)를 비슷하게 설정하겠습니다.

``` python
'''
<목표>
1. MLP_model에 MLP 모델을 생성하여 저장합니다. 첫번째 레이어의 출력 차원은 64, 두번째 레이어의 출력차원은 32, 마지막 레이어의 출력 차원은 10으로 합니다. 첫번째와 두번째 레이어의 활성함수는 relu로, 마지막 레이어의 활성함수는 softmax로 합니다.

2. CNN_model에 CNN 모델을 생성하여 저장합니다. 3개의 convolution 레이어와 2개의 dense 레이어로 구성합니다. 각 convolution layer의 필터 개수는 32, 커널 사이즈는 3 ×\times× 3으로 설정합니다.
'''
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import logging, os
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# 동일한 실행 결과 확인을 위한 코드입니다.
np.random.seed(123)
tf.random.set_seed(123)

# MNIST 데이터 세트를 불러옵니다.
mnist = tf.keras.datasets.mnist

# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    

# Train 데이터 5000개와 Test 데이터 500개를 사용합니다.
train_images, train_labels = train_images[:5000].astype(float), train_labels[:5000]
test_images, test_labels = test_images[:500].astype(float), test_labels[:500]

'''
1. 먼저 MLP 모델을 학습해보겠습니다.
'''
print('========== MLP ==========')

# MLP 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀 * 세로픽셀) 형태로 변환합니다.
train_images = tf.cast(tf.reshape(train_images, (5000, -1)) / 256., tf.float32)
train_labels = tf.convert_to_tensor(train_labels)
test_images = tf.cast(tf.reshape(test_images, (500, -1)) / 256., tf.float32)
test_labels = tf.convert_to_tensor(test_labels)

# MLP 모델을 설정합니다.
MLP_model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation = 'relu'),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dense(10, activation = 'softmax')
])

# MLP 모델의 학습 방법을 설정합니다.
MLP_model.compile(loss = 'sparse_categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])
              
# 학습을 수행합니다. 
history = MLP_model.fit(train_images, train_labels, epochs = 10, batch_size = 128, verbose = 2)

# MLP 모델 구조를 출력합니다. weight의 수가 52,650개입니다.
MLP_model.summary()

# 평가용 데이터를 활용하여 정확도를 평가합니다.
loss, test_acc = MLP_model.evaluate(test_images, test_labels, verbose = 0)

# 모델 평가 및 예측 결과를 출력합니다.
print('\nMLP Test Loss : {:.4f} | MLP Test Accuracy : {}\n'.format(loss, test_acc))

'''
2. 다음으로, CNN 모델을 학습해보겠습니다.
'''
print('========== CNN ==========')

# CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.
train_images = tf.reshape(train_images, (5000, 28, 28, 1))
test_images = tf.reshape(test_images, (500, 28, 28, 1))

# CNN 모델을 설정합니다.
CNN_model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME', input_shape = (28,28,1)),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),
    tf.keras.layers.MaxPool2D(padding = 'SAME'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation = 'relu'),
    tf.keras.layers.Dense(10, activation = 'softmax')
])

# CNN 모델의 학습 방법을 설정합니다.
CNN_model.compile(loss = 'sparse_categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

# 학습을 수행합니다. 
history = CNN_model.fit(train_images, train_labels, epochs = 10, batch_size = 128, verbose = 2)

# CNN 모델 구조를 출력합니다. weight의 수가 52,298개입니다.
CNN_model.summary()

# 평가용 데이터를 활용하여 정확도를 평가합니다.
loss, test_acc = CNN_model.evaluate(test_images, test_labels, verbose = 0)

# 모델 평가 및 예측 결과를 출력합니다.
print('\nCNN Test Loss : {:.4f} | CNN Test Accuracy : {}'.format(loss, test_acc))

'''
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
========== MLP ==========
(중략)
MLP Test Loss : 0.2255 | MLP Test Accuracy : 0.9319999814033508

========== CNN ==========
(중략)
CNN Test Loss : 0.0975 | CNN Test Accuracy : 0.9559999704360962
'''
```



**영화 리뷰 긍정/부정 분류 RNN모델**

- 이번 실습에서는 영화 리뷰 데이터를 바탕으로 **감정 분석**을 하는 모델을 학습 시켜 보겠습니다. 
- 영화 리뷰와 같은 **자연어 자료는 곧 단어의 연속적인 배열로써, 시계열 자료**라고 볼 수 있습니다. 즉, 시계열 자료(연속된 단어)를 이용해 리뷰에 내포된 감정(긍정, 부정)을 예측하는 분류기를 만들어 보겠습니다.
- 데이터셋은 **IMDB 영화 리뷰 데이터 셋**을 사용합니다. 훈련용 5,000개와 테스트용 1,000개로 이루어져 있으며, 레이블은 긍정/부정으로 두 가지입니다.

1. 데이터 전처리

> * 우선 자연어 데이터를 RNN 모델의 입력으로 사용할 수 있도록 데이터 전 처리를 수행해보겠습니다.
> * RNN을 위한 자연어 데이터 전 처리
>
> > * RNN의 입력으로 사용하기 위해서는 자연어 데이터는 토큰화 및 여러 가지의 데이터 처리가 필요합니다.
> > * 아래와 같이 자연어 데이터가 준비되어 있다면 마지막으로 패딩을 수행하여 데이터의 크기를 통일해야합니다.
> > * 첫 번째 데이터 시퀀스
> >
> > ![data_sequence](.\data_sequence.png)
> >
> > * 단어사전
> >
> > ![word_dictionary](.\word_dictionary.png)
> >
> > * 패딩
> >
> > ```python
> > sequence.pad_sequences(data, maxlen=300, padding='post')
> > ```
> >
> > > `data` 시퀀스의 크기가 `maxlen`인자보다 작으면 그 크기에 맞게 패딩을 추가합니다.

```python
'''
<목표>
인덱스로 변환된 X_train, X_test 시퀀스에 패딩을 수행하고 각각 X_train, X_test에 저장합니다.
시퀀스 최대 길이는 300으로 설정합니다.
'''
import json
import numpy as np
import tensorflow as tf
import data_process
from keras.datasets import imdb
from keras.preprocessing import sequence

import logging, os
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# 학습용 및 평가용 데이터를 불러오고 샘플 문장을 출력합니다.
X_train, y_train, X_test, y_test = data_process.imdb_data_load()

"""
1. 인덱스로 변환된 X_train, X_test 시퀀스에 패딩을 수행하고 각각 X_train, X_test에 저장합니다.
   시퀀스 최대 길이는 300으로 설정합니다.
"""
X_train = sequence.pad_sequences(X_train, maxlen=300, padding='post')
X_test = sequence.pad_sequences(X_test, maxlen=300, padding='post')

print("\n패딩을 추가한 첫 번째 X_train 데이터 샘플 토큰 인덱스 sequence: \n",X_train[0])

'''
Using TensorFlow backend.
첫 번째 X_train 데이터 샘플 문장: 
 the as you with out themselves powerful and and their becomes and had and of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every and and movie except her was several of enough more with is now and film as you of and and unfortunately of you than him that with out themselves her get for was and of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of and and with heart had and they of here that with her serious to have does when from why what have and they is you that isn't one will very to as itself with other and in of seen over and for anyone of and br and to whether from than out themselves history he name half some br of and and was two most of mean for 1 any an and she he should is thought and but of script you not while history he heart to real at and but when from one bit then have two of script their with her and most that with wasn't to with and acting watch an for with and film want an

첫 번째 X_train 데이터 샘플 토큰 인덱스 sequence: 
 [1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]
첫 번째 X_train 데이터 샘플 토큰 시퀀스 길이:  218
첫 번째 y_train 데이터:  1

패딩을 추가한 첫 번째 X_train 데이터 샘플 토큰 인덱스 sequence: 
 [  1  14  22  16  43 530 973   2   2  65 458   2  66   2   4 173  36 256
   5  25 100  43 838 112  50 670   2   9  35 480 284   5 150   4 172 112
 167   2 336 385  39   4 172   2   2  17 546  38  13 447   4 192  50  16
   6 147   2  19  14  22   4   2   2 469   4  22  71  87  12  16  43 530
  38  76  15  13   2   4  22  17 515  17  12  16 626  18   2   5  62 386
  12   8 316   8 106   5   4   2   2  16 480  66   2  33   4 130  12  16
  38 619   5  25 124  51  36 135  48  25   2  33   6  22  12 215  28  77
  52   5  14 407  16  82   2   8   4 107 117   2  15 256   4   2   7   2
   5 723  36  71  43 530 476  26 400 317  46   7   4   2   2  13 104  88
   4 381  15 297  98  32   2  56  26 141   6 194   2  18   4 226  22  21
 134 476  26 480   5 144  30   2  18  51  36  28 224  92  25 104   4 226
  65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19
 178  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0]
'''
```

2. 
